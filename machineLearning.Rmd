## Preparation

I'll use knitr for generating this report, caret for running the machine learning, lubridate to handle date data (if I need it), and doParallel to run computations in parallel when I can. I'll use caching in this document because the calculations can be very time-consuming. I'll also use functions from the randomForest library to allow additional cross-validation, and much better performance (speed) for the actual training, compared to the `method="rf"` in R's built in `train` function.

```{r}
library(knitr)
library(lubridate)
library(caret)
library(doParallel)
opts_chunk$set(cache=TRUE)

set.seed(989686)
```

I import the training 'as-is' from the given source---I will determine level variables manually: `classe` and `user_name`. Blank fields, and fields marked "NA" or "#DIV/0!" will all be treated as missing or NA values.

```{r}
training <- read.csv("pml-training.csv",
    na.strings=c("NA", "", "#DIV/0!"),
    as.is=TRUE)
training$classe <- factor(training$classe)
training$user_name <- factor(training$user_name)
```

I can also get rid of columns that are always NA, since there's obviously no usable data there.

```{r}
training <- training[,as.vector(colSums(is.na(training)) != length(training$X))]
```

### Split the data based on full or sparse data

There are a lot of columns where the data is NA most of the time, and only show usable data in about 200/19000 rows, or 1% of the time. This might be useful, but for now I'll try to get by with just the densely-populated data.

I split into two sets, "deep" where fewer columns but all 19k rows, and "wide", the much more limited set of rows where we have complete cases. In "wide", we remove a few more of the columns where the value is always the same (I'll also move `cvtd_timestamp` into a lubridate object in case I need to deal with time).

```{r}
fullCols <- as.vector(colSums(is.na(training)))==0 & as.vector(colSums(training==""))==0
training$cvtd_timestamp <- parse_date_time(training$cvtd_timestamp, "%d/%m/%Y %H:%M")

trDeep <- training[, fullCols]
trDeep <- trDeep[complete.cases(trDeep), ]
```

I'll set the 200 or so 'wide' rows with complete cases aside in their own table, `trWide`, and will then use a single binary indicator to track whether the row is a complete case or not in `trDeep`. I'll see how far I can get with just the narrow (deep) data and the binary indicator as a placeholder for wide rows. If I need the wide (shallow) data for more accuracy, I'll work on bringing it back in later.

```{r}
trWide <- training[complete.cases(training),]
trWide <- Filter(function(x)(length(unique(x))>1), trWide)

trDeep <- cbind(0,trDeep)
colnames(trDeep)[1] <- "wideRow"
trDeep$wideRow[trWide$X] <- 1
```

### Partitioning training and test data

I'll split training and test data 60/40. With that, I'm ready to proceed and try out a few algorithms.

```{r}
trvec <- createDataPartition(trDeep$classe, p=0.6, list=FALSE)
trDeep_tr <- trDeep[trvec,]
trDeep_ts <- trDeep[-trvec,]
```

### Which variables?

Most of the columns in `trDeep` are sensor data, except for several at the beginning. The non-sensor columns include `user_name`, timestamps, and what appears to be some programming overhead information (`new_window`, `num_window`). I'll leave most of these non-sensor variables out, since they are likely to contribute to over-fitting.

The grayest case is `user_name`. It seems likely that a user$+$sensor data combination is more powerful at prediction than sensor data alone, but I prefer to see how far I can do with the motion data alone first, and then see if I want to bring the user information back in if I need it.

Since `wideRow` is a placeholder for sensor data, I'll include it and check its importance after the first training attempt.


### Try 1: Random Forest

I start by running the training on the 'deep' training set, `trDeep_tr`, with only sensor data (including the `wideRow` placeholder).

"Random forest" is the default training method in caret. It is reportedly very good for predictive accuracy. The disadvantage is speed and over-fitting. I'll throw two cores at it to address the speed issue, and we'll use simple cross-validation to address the over-fitting.

```{r}
registerDoParallel(cores=2)
trainedRf <- randomForest(classe ~., data=trDeep_tr[,c(-2:-8)])
```

I can measure in-model accuracy by checking the confusion matrix against the training set:

```{r}
cmRfIn <- confusionMatrix(trDeep_tr$classe, predict(trainedRf, trDeep_tr[,c(-2:-8)]))
```

Random forest was able to reach a perfect in-model fit (accuracy `r I(cmRfIn$overall["Accuracy"])`) with the training data. Now let's check out-of-sample error with cross-validation against the (pre-)test set `trDeep_ts`:

```{r}
predictRf <- predict(trainedRf, trDeep_ts[,c(-2:-8)])
cmRf <- confusionMatrix(trDeep_ts$classe, predictRf)
cmRf
```

This accuracy of `cmRf$overall[1]` might be as good as it gets, but we'll try out a few other algorithms for comparison, to see if we can beat it.

### Aside: significance of `wideRow`s

I'll check the importance of each variable used, including `wideRow`, using varImp:

```{r}
variableImportance <- varImp(trainedRf)
variableImportance["wideRow",]
```

For the sake of comparison, here are the next few importance rankings:

```{r}
sort(variableImportance$Overall)[1:7]
```

It looks like `wideRow` is about 1% as influential as the next-least important variable, so I will continue with the data in `trDeep` only.

### Try 2: Boosting

The spatial-position-to-classification element of boosting makes it a good candidate for comparison. On `trDeep`, this amounts to locating a point in a 50-dimensional space and classifying it based on its position. It feels like a good fit intuitively.

```{r}
trainedGbm <- train(classe ~., data=trDeep_tr[,c(-1:-8)], method="gbm", verbose=FALSE)
```

Now look at the in-sample error:

```{r}
cmGbmIn <- confusionMatrix(trDeep_tr$classe, predict(trainedGbm, trDeep_tr[,c(-2:-8)]))
cmGbmIn
```


And check out-of-sample error by cross-validating against the (pre-)testing data:

```{r}
predictGbm <- predict(trainedGbm, trDeep_ts[,c(-1:-8)])
cmGbm <- confusionMatrix(trDeep_ts$classe, predictGbm)
cmGbm
```

The accuracy is also quite good, but random forest still looks better.


### Try 3: `rpart` regression/classification trees

It might be possible to use R's default tree-based method.

```{r}
registerDoParallel(cores=1)
trainedRpart <- train(classe ~., data=trDeep_tr[,c(-1:-8)], method="rpart")
```

Again, look at the in-sample error:

```{r}
cmRpartIn <- confusionMatrix(trDeep_tr$classe, predict(trainedRpart, trDeep_tr[,c(-2:-8)]))
cmRpartIn
```

and out-of-sample error with cross-validation:

```{r}
predictRpart <- predict(trainedRpart, trDeep_ts[,c(-1:-8)])
cmRpart <- confusionMatrix(trDeep_ts$classe, predictRpart)
cmRpart
```

It's almost a relief to find that there are indeed methods which won't work as well. The lower accuracy takes it out of the running.


## Final prediction

It looks like random forest with default configuration should be sufficient to make the final prediction on the test data. I'll run the training on the entire `trDeep` data set:

```{r}
finalTrainRf <- randomForest(classe ~., data=trDeep[,c(-1:-8)])
```

Now, I'll import the test data I'll need to make the prediction, with the same parameters, and I'll make sure to select the same columns.

```{r}
testdata <- read.csv("pml-testing.csv",
    na.strings=c("NA", "", "#DIV/0!"),
    as.is=TRUE)
testdata$user_name <- factor(testdata$user_name)
# add "0" as wideRow, just so they line up
testdata <- cbind(0, testdata)
colnames(testdata)[1] <- "wideRow"
testdata <- testdata[,colnames(trDeep)[-length(trDeep)]]
finalPredict <- predict(finalTrainRf, testdata[,c(-1:-8)])
```

The results are as follows:

```{r}
finalPredict
```

And for good measure, I'll double check that the boost model from training set performs similarly. If there is more than one difference between the two predictions, it means I made an incorrect assumption---the gbm accuracy of about 95% implies I could expect to see one difference.

```{r}
sum((predict(finalTrainRf, testdata[,c(-1,-8)]) == predict(trainedGbm, testdata[,c(-2:-8)])))
```

If the result here is 19 or 20, I'm confident enough to submit the results of the randomForest training as my final prediction.

