## Preparation

I'll use knitr for generating this report, caret for running the machine learning, lubridate to handle date data (if I need it), and doParallel to run computations in parallel when I can. I'll use caching in this document because the calculations can be very time-consuming. I'll also use functions from the randomForest library to allow additional cross-validation, and much better performance (speed) for the actual training, compared to the `method="rf"` in R's built in `train` function.

```{r}
library(knitr)
library(lubridate)
library(caret)
library(doParallel)
opts_chunk$set(cache=TRUE)

set.seed(989686)
```

I import the training 'as-is' from the given source---I will determine level variables manually: `classe` and `user_name`. Blank fields, and fields marked "NA" or "#DIV/0!" will all be treated as missing or NA values.

```{r}
training <- read.csv("pml-training.csv",
    na.strings=c("NA", "", "#DIV/0!"),
    as.is=TRUE)
training$classe <- factor(training$classe)
training$user_name <- factor(training$user_name)
```

I can also get rid of columns that are always NA, since there's obviously no usable data there.

```{r}
training <- training[,as.vector(colSums(is.na(training)) != length(training$X))]
```

### Split the data based on full or sparse data

There are a lot of columns where the data is NA most of the time, and only show usable data in about 200/19000 rows, or 1% of the time. This might be useful, but for now I'll try to get by with just the densely-populated data.

I split into two sets, "deep" where fewer columns but all 19k rows, and "wide", the much more limited set of rows where we have complete cases. In "wide", we remove a few more of the columns where the value is always the same (I'll also move `cvtd_timestamp` into a lubridate object in case I need to deal with time).

```{r}
fullCols <- as.vector(colSums(is.na(training)))==0 & as.vector(colSums(training==""))==0
training$cvtd_timestamp <- parse_date_time(training$cvtd_timestamp, "%d/%m/%Y %H:%M")

trDeep <- training[, fullCols]
trDeep <- trDeep[complete.cases(trDeep), ]
```

I'll set the 200 or so 'wide' rows with complete cases aside in their own table, `trWide`, and will then use a single binary indicator to track whether the row is a complete case or not in `trDeep`. I'll see how far I can get with just the narrow (deep) data and the binary indicator as a placeholder for wide rows. If I need the wide (shallow) data for more accuracy, I'll work on bringing it back in later.

```{r}
trWide <- training[complete.cases(training),]
trWide <- Filter(function(x)(length(unique(x))>1), trWide)

trDeep <- cbind(0,trDeep)
colnames(trDeep)[1] <- "wideRow"
trDeep$wideRow[trWide$X] <- 1
```

### Partitioning training and test data

I'll split training and test data 60/40. With that, I'm ready to proceed and try out a few algorithms.

```{r}
trvec <- createDataPartition(trDeep$classe, p=0.6, list=FALSE)
trDeep_tr <- trDeep[trvec,]
trDeep_ts <- trDeep[-trvec,]
```

### Which variables?

Most of the columns in `trDeep` are sensor data, except for several at the beginning. The non-sensor columns include `user_name`, timestamps, and what appears to be some programming overhead information (`new_window`, `num_window`). I'll leave most of these non-sensor variables out, since they are likely to contribute to over-fitting.

The grayest case is `user_name`. It seems likely that a user$\+$sensor data combination is more powerful at prediction than sensor data alone, but I prefer to see how far I can do with the motion data alone first, and then see if I want to bring the user information back in if I need it.

Since `wideRow` is a placeholder for sensor data, I'll include it and check its importance after the first training attempt.


### Try 1: Random Forest

I start by running the training on the 'deep' training set, `trDeep_tr`, with only sensor data (including the `wideRow` placeholder).

"Random forest" is the default training method in caret. It is reportedly very good for predictive accuracy. The disadvantage is speed and over-fitting. I'll throw two cores at it to address the speed issue, and we'll use simple cross-validation to address the over-fitting.

```{r}
registerDoParallel(cores=2)
trainedRf <- randomForest(classe ~., data=trDeep_tr[,c(-2:-8)])
predictRf <- predict(trainedRf, trDeep_ts[,c(-2:-8)])
```

So then, cross-validation against the test set `trDeep_ts`:

```{r}
cmRf <- confusionMatrix(trDeep_ts$classe, predictRf)
cmRf
```

This accuracy of `cmRf$overall[1]` might be as good as it gets, but we'll try out a few other algorithms for comparison, to see if we can beat it.

### Aside: significance of `wideRow`s

We can check the importance of each variable used, including `wideRow`, using varImp:

```{r}
variableImportance <- varImp(trainedRf)
```

With such a low importance, we can continue with the data in `trDeep` only.

### Try 2: Boosting

The spatial-position-to-classification element of boosting makes it a good candidate for comparison. On `trDeep`, this amounts to locating a point in a 50-dimensional space and classifying it based on its position. It feels like a good fit intuitively.

```{r}
trainedGbm <- train(classe ~., data=trDeep_tr[,c(-1:-8)], method="gbm", verbose=FALSE)
predictGbm <- predict(trainedGbm, trDeep_ts[,c(-1:-8)])
```

I'll cross-validate against the (pre-)testing data and show the confusion matrix for comparison:

```{r}
cmGbm <- confusionMatrix(trDeep_ts$classe, predictGbm)
cmGbm
```

The accuracy is also quite good, but random forest still looks better.


### Try 3: `rpart` regression/classification trees

It might be possible to use R's default tree-based method.

```{r}
registerDoParallel(cores=1)
trainedRpart <- train(classe ~., data=trDeep_tr[,c(-1:-8)], method="rpart")
predictRpart <- predict(trainedRpart, trDeep_ts[,c(-1:-8)])
```


```{r}
cmRpart <- confusionMatrix(trDeep_ts$classe, predictRpart)
cmRpart
```

It's almost a relief to find that there are indeed methods which won't work as well. The lower accuracy takes it out of the running.

## Additional cross-validation

I'm nearly convinced to proceed with the out-of-the-box random forest method. The cross-validation between the training and pre-testing set shows an accuracy of around 99%. If I can verify this with an additional cross-validation, I'll proceed with it. I'll use the included `rfvc` function:

```{r}

checkrf <- rfcv(trDeep_tr[,c(-2:-8)], predictRf)

```







